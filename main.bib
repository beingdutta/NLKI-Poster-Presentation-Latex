% Long form of conference & journal abbreviations -- especially for camera ready
@String(PAMI  = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV  = {Int. J. Comput. Vis.})
@String(CVPR  = {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV  = {Int. Conf. Comput. Vis.})
@String(ECCV  = {Eur. Conf. Comput. Vis.})
@String(NeurIPS = {Adv. Neural Inform. Process. Syst.})
@String(ICML  = {Int. Conf. Mach. Learn.})
@String(ICLR  = {Int. Conf. Learn. Represent.})
@String(ACCV  = {Asian Conf. Comput. Vis.})
@String(BMVC  = {Brit. Mach. Vis. Conf.})
@String(CVPRW = {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(AAAI  = {AAAI})
@String(IJCAI = {IJCAI})
@String(ICIP  = {IEEE Int. Conf. Image Process.})
@String(ICPR  = {Int. Conf. Pattern Recog.})
@String(ICASSP=	{ICASSP})
@String(ICME  = {Int. Conf. Multimedia and Expo})
@String(JMLR  = {J. Mach. Learn. Res.})
@String(TMLR  = {Trans. Mach. Learn Res.})
@String(TOG   = {ACM Trans. Graph.})
@String(TIP   = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TCSVT = {IEEE Trans. Circuit Syst. Video Technol.})
@String(TMM   = {IEEE Trans. Multimedia})
@String(ACMMM = {ACM Int. Conf. Multimedia})
@String(PR    = {Pattern Recognition})

@String(MNI	  = {Nature Mach. Intell.})
@String(SPL	  = {IEEE Sign. Process. Letters})
@String(VR    = {Vis. Res.})
@String(JOV	  = {J. Vis.})
@String(TVC   = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF   = {Comput. Graph. Forum})
@String(CVM   = {Computational Visual Media})


% Short form of conference & journal abbreviations -- especially for submission version
% if desired, remove these macros in favor of the above ones
@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NeurIPS = {NeurIPS})
@String(ICML  = {ICML})
@String(ICLR  = {ICLR})
@String(ACCV  = {ACCV})
@String(BMVC  =	{BMVC})
@String(CVPRW = {CVPRW})
@String(AAAI  = {AAAI})
@String(IJCAI = {IJCAI})
@String(ICIP  = {ICIP})
@String(ICPR  = {ICPR})
@String(ICASSP=	{ICASSP})
@String(ICME  =	{ICME})
@String(JMLR  = {JMLR})
@String(TMLR  = {TMLR})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(PR    = {PR})


@inproceedings{vilt1,
  title={Vilt: Vision-and-language transformer without convolution or region supervision},
  author={Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
  booktitle={International Conference on Machine Learning},
  pages={5583--5594},
  year={2021},
  organization={PMLR}
}
@article{cric,
  title={CRIC: A VQA Dataset for Compositional Reasoning on Vision and Commonsense},
  author={Gao, Difei and Wang, Ruiping and Shan, Shiguang and Chen, Xilin},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023},
  volume={45},
  number={5},
  pages={5561-5578},
  doi={10.1109/TPAMI.2022.3210780}}
}
@article{snlive,
  title={Visual Entailment: A Novel Task for Fine-grained Image Understanding},
  author={Xie, Ning and Lai, Farley and Doran, Derek and Kadav, Asim},
  journal={arXiv preprint arXiv:1901.06706},
  year={2019}
}
@article{commonsenseqa,
  title={Commonsenseqa: A question answering challenge targeting commonsense knowledge},
  author={Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
  journal={arXiv preprint arXiv:1811.00937},
  year={2018}
}

@InProceedings{vqa,
    author = {Yash Goyal and Tejas Khot and Douglas Summers{-}Stay and Dhruv Batra and Devi Parikh},
    title = {Making the {V} in {VQA} Matter: Elevating the Role of Image Understanding in {V}isual {Q}uestion {A}nswering},
    booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
    year = {2017},
}
@inproceedings{vcr,
  author = {Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  title = {From Recognition to Cognition: Visual Commonsense Reasoning},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2019}
}


@article{natarajan2013learning,
  title={Learning with noisy labels},
  author={Natarajan, Nagarajan and Dhillon, Inderjit S and Ravikumar, Pradeep K and Tewari, Ambuj},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{zhou2018nonGCE,
  title={Non-vacuous generalization bounds at the imagenet scale: a PAC-bayesian compression approach},
  author={Zhou, Wenda and Veitch, Victor and Austern, Morgane and Adams, Ryan P and Orbanz, Peter},
  journal={arXiv preprint arXiv:1804.05862},
  year={2018}
}

@inproceedings{ma2020normalizedActivePassiveLoss,
  title={Normalized loss functions for deep learning with noisy labels},
  author={Ma, Xingjun and Huang, Hanxun and Wang, Yisen and Romano, Simone and Erfani, Sarah and Bailey, James},
  booktitle={International conference on machine learning},
  pages={6543--6553},
  year={2020},
  organization={PMLR}
}

@article{huang2020selfadaptive,
  title={Self-adaptive training: beyond empirical risk minimization},
  author={Huang, Lang and Zhang, Chao and Zhang, Hongyang},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={19365--19376},
  year={2020}
}

@article{zhang2018generalizedLQloss,
  title={Generalized cross entropy loss for training deep neural networks with noisy labels},
  author={Zhang, Zhilu and Sabuncu, Mert},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{li2021improvedREGSL,
  title={Improved regularization and robustness for fine-tuning in neural networks},
  author={Li, Dongyue and Zhang, Hongyang},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={27249--27262},
  year={2021}
}

@article{ragRetriever,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio, and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike, and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{reltr,
  title={Reltr: Relation transformer for scene graph generation},
  author={Cong, Yuren and Yang, Michael Ying and Rosenhahn, Bodo},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023},
  publisher={IEEE}
}

@article{passageRetrieval,
  title={Leveraging passage retrieval with generative models for open domain question answering},
  author={Izacard, Gautier and Grave, Edouard},
  journal={arXiv preprint arXiv:2007.01282},
  year={2020}
}

@article{retreivalFromWiki,
  title={Reading Wikipedia to answer open-domain questions},
  author={Chen, Danqi and Fisch, Adam and Weston, Jason and Bordes, Antoine},
  journal={arXiv preprint arXiv:1704.00051},
  year={2017}
}

@article{knowledgeIntegrationImgCaption,
    title = {External knowledge-assisted Transformer for image captioning},
    journal = {Image and Vision Computing},
    volume = {140},
    pages = {104864},
    year = {2023},
    issn = {0262-8856},
    doi = {https://doi.org/10.1016/j.imavis.2023.104864},
    url = {https://www.sciencedirect.com/science/article/pii/S026288562300238X},
    author = {Zhixin Li and Qiang Su and Tianyu Chen},
    keywords = {Image captioning, Knowledge reasoning, Object relation, Visual Transformer},
}

@misc{faiss,
      title={The Faiss library}, 
      author={Matthijs Douze and Alexandr Guzhva and Chengqi Deng and Jeff Johnson and Gergely Szilvasy and Pierre-Emmanuel Mazaré and Maria Lomeli and Lucas Hosseini and Hervé Jégou},
      year={2024},
      eprint={2401.08281},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.08281}, 
}

@misc{li2022survey,
      title={A Survey on Retrieval-Augmented Text Generation}, 
      author={Huayang Li and Yixuan Su and Deng Cai and Yan Wang and Lemao Liu},
      year={2022},
      eprint={2202.01110},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{li2019visualbert,
      title={VisualBERT: A Simple and Performant Baseline for Vision and Language}, 
      author={Liunian Harold Li and Mark Yatskar and Da Yin and Cho-Jui Hsieh and Kai-Wei Chang},
      year={2019},
      eprint={1908.03557},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{raco,
    title = "Retrieval Augmentation for Commonsense Reasoning: A Unified Approach",
    author = "Yu, Wenhao  and
      Zhu, Chenguang  and
      Zhang, Zhihan  and
      Wang, Shuohang  and
      Zhang, Zhuosheng  and
      Fang, Yuwei  and
      Jiang, Meng",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.294",
    doi = "10.18653/v1/2022.emnlp-main.294",
    pages = "4364--4377",
    abstract = "A common thread of retrieval-augmented methods in the existing literature focuses on retrieving encyclopedic knowledge, such as Wikipedia, which facilitates well-defined entity and relation spaces that can be modeled. However, applying such methods to commonsense reasoning tasks faces two unique challenges, i.e., the lack of a general large-scale corpus for retrieval and a corresponding effective commonsense retriever. In this paper, we systematically investigate how to leverage commonsense knowledge retrieval to improve commonsense reasoning tasks. We proposed a unified framework of retrieval-augmented commonsense reasoning (called RACo), including a newly constructed commonsense corpus with over 20 million documents and novel strategies for training a commonsense retriever. We conducted experiments on four different commonsense reasoning tasks. Extensive evaluation results showed that our proposed RACo can significantly outperform other knowledge-enhanced method counterparts, achieving new SoTA performance on the CommonGen and CREAK leaderboards.",
}

@misc{fvqa2018,
      title={FVQA: Fact-based Visual Question Answering}, 
      author={Peng Wang and Qi Wu and Chunhua Shen and Anton van den Hengel and Anthony Dick},
      year={2017},
      eprint={1606.05433},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1606.05433}, 
}

@InProceedings{okvqaMarino_2019_CVPR,
author = {Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
title = {OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@article{kbvqawang2015explicit,
  title={Explicit knowledge-based reasoning for visual question answering},
  author={Wang, Peng and Wu, Qi and Shen, Chunhua and Hengel, Anton van den and Dick, Anthony},
  journal={arXiv preprint arXiv:1511.02570},
  year={2015}
}

@inproceedings{aditya2019integrating,
  title={Integrating knowledge and reasoning in image understanding},
  author={Aditya, Somak and Yang, Yezhou and Baral, Chitta},
  booktitle={28th International Joint Conference on Artificial Intelligence, IJCAI 2019},
  pages={6252--6259},
  year={2019},
  organization={International Joint Conferences on Artificial Intelligence}
}

@misc{aditya2018explicit,
      title={Explicit Reasoning over End-to-End Neural Architectures for Visual Question Answering}, 
      author={Somak Aditya and Yezhou Yang and Chitta Baral},
      year={2018},
      eprint={1803.08896},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1803.08896}, 
}

@article{kvqaShah_Mishra_Yadati_Talukdar_2019, 
title={KVQA: Knowledge-Aware Visual Question Answering}, 
volume={33}, 
url={https://ojs.aaai.org/index.php/AAAI/article/view/4915}, 
DOI={10.1609/aaai.v33i01.33018876}, abstractNote={&lt;p&gt;Visual Question Answering (VQA) has emerged as an important problem spanning Computer Vision, Natural Language Processing and Artificial Intelligence (AI). In conventional VQA, one may ask questions about an image which can be answered purely based on its content. For example, given an image with people in it, a typical VQA question may inquire about the number of people in the image. More recently, there is growing interest in answering questions which require &lt;em&gt;commonsense knowledge&lt;/em&gt; involving &lt;em&gt;common nouns&lt;/em&gt; (e.g., cats, dogs, microphones) present in the image. In spite of this progress, the important problem of answering questions requiring &lt;em&gt;world knowledge&lt;/em&gt; about &lt;em&gt;named entities&lt;/em&gt; (e.g., Barack Obama, White House, United Nations) in the image has not been addressed in prior research. We address this gap in this paper, and introduce KVQA – the first dataset for the task of (world) knowledge-aware VQA. KVQA consists of 183K question-answer pairs involving more than 18K named entities and 24K images. Questions in this dataset require multi-entity, multi-relation, and multi-hop reasoning over large Knowledge Graphs (KG) to arrive at an answer. To the best of our knowledge, KVQA is the largest dataset for exploring VQA over KG. Further, we also provide baseline performances using state-of-the-art methods on KVQA.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Shah, Sanket and Mishra, Anand and Yadati, Naganand and Talukdar, Partha Pratim}, year={2019}, month={Jul.}, pages={8876-8884} }

@inproceedings{ye2023improvingdance,
  title={Improving Commonsense in Vision-Language Models via Knowledge Graph Riddles},
  author={Ye, Shuquan and Xie, Yujia and Chen, Dongdong and Xu, Yichong and Yuan, Lu and Zhu, Chenguang and Liao, Jing},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2634--2645},
  year={2023}
}

@article{Lee2021VisionLanguageKnowledgeCF,
  title={Vision–Language–Knowledge Co-Embedding for Visual Commonsense Reasoning},
  author={Jaeyun Lee and Incheol Kim},
  journal={Sensors (Basel, Switzerland)},
  year={2021},
  volume={21},
  url={https://api.semanticscholar.org/CorpusID:233462391}
}

@article{ZHENG2021108153,
title = {Knowledge base graph embedding module design for Visual question answering model},
journal = {Pattern Recognition},
volume = {120},
pages = {108153},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108153},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100340X},
author = {Wenfeng Zheng and Lirong Yin and Xiaobing Chen and Zhiyang Ma and Shan Liu and Bo Yang},
keywords = {Faster R-CNN, DBpedia spotlight, knowledge base, VQA},
}

@misc{chen2021zeroshot,
      title={Zero-shot Visual Question Answering using Knowledge Graph}, 
      author={Zhuo Chen and Jiaoyan Chen and Yuxia Geng and Jeff Z. Pan and Zonggang Yuan and Huajun Chen},
      year={2021},
      eprint={2107.05348},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{karpukhin2020dense,
      title={Dense Passage Retrieval for Open-Domain Question Answering}, 
      author={Vladimir Karpukhin and Barlas Oğuz and Sewon Min and Patrick Lewis and Ledell Wu and Sergey Edunov and Danqi Chen and Wen-tau Yih},
      year={2020},
      eprint={2004.04906},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{cong2023reltr,
  title={Reltr: Relation transformer for scene graph generation},
  author={Cong, Yuren and Yang, Michael Ying and Rosenhahn, Bodo},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023},
  publisher={IEEE}
}

@article{chen2024knowledge,
  author       = {Zhuo Chen and
                  Yichi Zhang and
                  Yin Fang and
                  Yuxia Geng and
                  Lingbing Guo and
                  Xiang Chen and
                  Qian Li and
                  Wen Zhang and
                  Jiaoyan Chen and
                  Yushan Zhu and
                  Jiaqi Li and
                  Xiaoze Liu and
                  Jeff Z. Pan and
                  Ningyu Zhang and
                  Huajun Chen},
  title        = {Knowledge Graphs Meet Multi-Modal Learning: {A} Comprehensive Survey},
  journal      = {CoRR},
  volume       = {abs/2402.05391},
  year         = {2024}
}

@inproceedings{lin-byrne-2022-retrieval-ravqa,
    title = "Retrieval Augmented Visual Question Answering with Outside Knowledge",
    author = "Lin, Weizhe  and
      Byrne, Bill",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.772",
    doi = "10.18653/v1/2022.emnlp-main.772",
    pages = "11238--11254",
    abstract = "Outside-Knowledge Visual Question Answering (OK-VQA) is a challenging VQA task that requires retrieval of external knowledge to answer questions about images. Recent OK-VQA systems use Dense Passage Retrieval (DPR) to retrieve documents from external knowledge bases, such as Wikipedia, but with DPR trained separately from answer generation, introducing a potential limit on the overall system performance. Instead, we propose a joint training scheme which includes differentiable DPR integrated with answer generation so that the system can be trained in an end-to-end fashion. Our experiments show that our scheme outperforms recent OK-VQA systems with strong DPR for retrieval. We also introduce new diagnostic metrics to analyze how retrieval and generation interact. The strong retrieval ability of our model significantly reduces the number of retrieved documents needed in training, yielding significant benefits in answer quality and computation required for training.",
}

@misc{visdial,
      title={Visual Dialog}, 
      author={Abhishek Das and Satwik Kottur and Khushi Gupta and Avi Singh and Deshraj Yadav and José M. F. Moura and Devi Parikh and Dhruv Batra},
      year={2017},
      eprint={1611.08669},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1611.08669}, 
}

@inproceedings{yang-etal-2023-vilm,
    title = "Re-{V}i{LM}: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning",
    author = "Yang, Zhuolin  and
      Ping, Wei  and
      Liu, Zihan  and
      Korthikanti, Vijay  and
      Nie, Weili  and
      Huang, De-An  and
      Fan, Linxi  and
      Yu, Zhiding  and
      Lan, Shiyi  and
      Li, Bo  and
      Shoeybi, Mohammad  and
      Liu, Ming-Yu  and
      Zhu, Yuke  and
      Catanzaro, Bryan  and
      Xiao, Chaowei  and
      Anandkumar, Anima",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.793",
    doi = "10.18653/v1/2023.findings-emnlp.793",
    pages = "11844--11857",
    abstract = "Augmenting pretrained language models (LMs) with a vision encoder (e.g., Flamingo) has obtained state-of-the-art results in image-to-text generation. However, these models store all the knowledge within their parameters, thus often requiring enormous model parameters to model the abundant visual concepts and very rich text descriptions. Additionally, they are inefficient in incorporating new data, requiring a computational-expensive fine-tuning process. In this work, we introduce a Retrieval-augmented Visual Language Model, Re-ViLM, built upon the Flamingo, that supports retrieving the relevant knowledge from the external database for zero and in-context few-shot image-to-text generations. By storing certain knowledge explicitly in the external database, our approach reduces the number of model parameters and can easily accommodate new data during evaluation by simply updating the database. We also construct an interleaved image and text data that facilitates in-context few-shot learning capabilities.We demonstrate that Re-ViLM significantly boosts performance for image-to-text generation tasks, especially for zero-shot and few-shot generation in out-of-domain settings with 4x less parameters compared with baseline methods.",
}

@inproceedings{ghosal-etal-2023-language,
    title = "Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts",
    author = "Ghosal, Deepanway  and
      Majumder, Navonil  and
      Lee, Roy  and
      Mihalcea, Rada  and
      Poria, Soujanya",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.809",
    doi = "10.18653/v1/2023.findings-emnlp.809",
    pages = "12096--12102",
    abstract = "Visual question answering (VQA) is the task of answering questions about an image. The task assumes an understanding of both the image and the question to provide a natural language answer. VQA has gained popularity in recent years due to its potential applications in a wide range of fields, including robotics, education, and healthcare. In this paper, we focus on knowledge-augmented VQA, where answering the question requires commonsense knowledge, world knowledge, and reasoning about ideas and concepts not present in the image. We propose a multimodal framework that uses language guidance (LG) in the form of rationales, image captions, scene graphs, etc to answer questions more accurately. We benchmark our method on the multi-choice question-answering task of the A-OKVQA, Science-QA, VSR, and IconQA datasets using CLIP and BLIP models. We show that the use of language guidance is a simple but powerful and effective strategy for visual question answering. Our language guidance improves the performance of CLIP by 7.6{\%} and BLIP-2 by 4.8{\%} in the challenging A-OKVQA dataset. We also observe consistent improvement in performance on the Science-QA, VSR, and IconQA datasets when using the proposed language guidances. The implementation of LG-VQA is publicly available at https://github.com/declare-lab/LG-VQA.",
}

@inproceedings{singh2022flava,
  author = {
    Amanpreet Singh and
    Ronghang Hu and
    Vedanuj Goswami and
    Guillaume Couairon and
    Wojciech Galuba and
    Marcus Rohrbach and
    Douwe Kiela
  },
  title = {
    {FLAVA:} {A} Foundational Language And 
    Vision Alignment Model
  },
  booktitle={CVPR},
  year={2022}
}

@misc{blip2,
      title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models}, 
      author={Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
      year={2023},
      eprint={2301.12597},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2301.12597}, 
}

@misc{yolo,
      title={You Only Look Once: Unified, Real-Time Object Detection}, 
      author={Joseph Redmon and Santosh Divvala and Ross Girshick and Ali Farhadi},
      year={2016},
      eprint={1506.02640},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1506.02640}, 
}

@misc{colbert,
      title={ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT}, 
      author={Omar Khattab and Matei Zaharia},
      year={2020},
      eprint={2004.12832},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2004.12832}, 
}

@misc{sbert,
      title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks}, 
      author={Nils Reimers and Iryna Gurevych},
      year={2019},
      eprint={1908.10084},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1908.10084}, 
}

@inproceedings{ruletakerclark20,
author = {Clark, Peter and Tafjord, Oyvind and Richardson, Kyle},
title = {Transformers as soft reasoners over language},
year = {2021},
isbn = {9780999241165},
abstract = {Beginning with McCarthy's Advice Taker (1959), AI has pursued the goal of providing a system with explicit, general knowledge and having the system reason over that knowledge. However, expressing the knowledge in a formal (logical or probabilistic) representation has been a major obstacle to this research. This paper investigates a modern approach to this problem where the facts and rules are provided as natural language sentences, thus bypassing a formal representation. We train transformers to reason (or emulate reasoning) over these sentences using synthetically generated data. Our models, that we call RuleTakers, provide the first empirical demonstration that this kind of soft reasoning over language is learnable, can achieve high (99\%) accuracy, and generalizes to test data requiring substantially deeper chaining than seen during training (95\%+ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as limited "soft theorem provers" operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering.},
booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
articleno = {537},
numpages = {9},
location = {Yokohama, Yokohama, Japan},
series = {IJCAI'20}
}

@inproceedings{omcshavasi2010,
author = {Havasi, Catherine and Speer, Robert and Arnold, Kenneth and Lieberman, Henry and Alonso, Jason and Moeller, Jesse},
title = {Open mind common sense: crowd-sourcing for common sense},
year = {2010},
publisher = {AAAI Press},
abstract = {Open Mind Common Sense (OMCS) is a freely available crowd-sourced knowledge base of natural language statements about the world. The goal of Open Mind Common Sense is to provide intuition to AI systems and applications by giving them access to a broad collection of basic information and the computational tools to work with this data. For our system demo, we will be presenting three aspects of the OMCS project: the OMCS knowledge base, the Concept-Net semantic network (Liu and Singh 2004) (Havasi, Speer, and Alonso 2007), and the AnalogySpace algorithm (Speer, Havasi, and Lieberman 2008) which deals well with noisy, user-contributed data.},
booktitle = {Proceedings of the 2nd AAAI Conference on Collaboratively-Built Knowledge Sources and Artificial Intelligence},
pages = {53},
numpages = {1},
series = {AAAIWS'10-02}
}

@article{llmaskbs2024lapata,
  author       = {Danna Zheng and
                  Mirella Lapata and
                  Jeff Z. Pan},
  title        = {Large Language Models as Reliable Knowledge Bases?},
  journal      = {CoRR},
  volume       = {abs/2407.13578},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2407.13578},
  doi          = {10.48550/ARXIV.2407.13578},
  eprinttype    = {arXiv},
  eprint       = {2407.13578},
  timestamp    = {Thu, 22 Aug 2024 15:43:20 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2407-13578.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{rao2024raven,
  title={RAVEN: Multitask Retrieval Augmented Vision-Language Learning},
  author={Rao, Varun Nagaraj and Choudhary, Siddharth and Deshpande, Aditya and Satzoda, Ravi Kumar and Appalaraju, Srikar},
  journal={arXiv preprint arXiv:2406.19150},
  year={2024}
}

@inproceedings{
wang2023visuallyaugmented,
title={Visually-Augmented Language Modeling},
author={Weizhi Wang and Li Dong and Hao Cheng and Haoyu Song and Xiaodong Liu and Xifeng Yan and Jianfeng Gao and Furu Wei},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=8IN-qLkl215}
}

@inproceedings{hu2023reveal,
  title={Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory},
  author={Hu, Ziniu and Iscen, Ahmet and Sun, Chen and Wang, Zirui and Chang, Kai-Wei and Sun, Yizhou and Schmid, Cordelia and Ross, David A and Fathi, Alireza},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={23369--23379},
  year={2023}
}

@inproceedings{kat-framework,
  title={KAT: A Knowledge Augmented Transformer for Vision-and-Language},
  author={Gui, Liangke and Wang, Borui and Huang, Qiuyuan and Hauptmann, Alexander G and Bisk, Yonatan and Gao, Jianfeng},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={956--968},
  year={2022}
}

@inproceedings{yasunaga2023retrieval,
  title={Retrieval-Augmented Multimodal Language Modeling},
  author={Yasunaga, Michihiro and Aghajanyan, Armen and Shi, Weijia and James, Richard and Leskovec, Jure and Liang, Percy and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-Tau},
  booktitle={International Conference on Machine Learning},
  pages={39755--39769},
  year={2023},
  organization={PMLR}
}

@inproceedings{caffagni2024wiki,
  title={Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs},
  author={Caffagni, Davide and Cocchi, Federico and Moratelli, Nicholas and Sarto, Sara and Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1818--1826},
  year={2024}
}

@inproceedings{wang2022ofa,
  title={Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework},
  author={Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
  booktitle={International conference on machine learning},
  pages={23318--23340},
  year={2022},
  organization={PMLR}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{rao2023retrieval,
  title={Retrieval-based knowledge augmented vision language pre-training},
  author={Rao, Jiahua and Shan, Zifei and Liu, Longpo and Zhou, Yao and Yang, Yuedong},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={5399--5409},
  year={2023}
}

@inproceedings{mensink2023encyclopedic,
  title={Encyclopedic VQA: Visual questions about detailed properties of fine-grained categories},
  author={Mensink, Thomas and Uijlings, Jasper and Castrejon, Lluis and Goel, Arushi and Cadar, Felipe and Zhou, Howard and Sha, Fei and Araujo, Andr{\'e} and Ferrari, Vittorio},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3113--3124},
  year={2023}
}

@article{liu2024prismer,
    title={Prismer: A Vision-Language Model with Multi-Task Experts},
    author={Liu, Shikun and Fan, Linxi and Johns, Edward and Yu, Zhiding and Xiao, Chaowei and Anandkumar, Anima},
    journal={Transactions on Machine Learning Research},
    year={2024}
}

@misc{izacard2021contriever,
  title={Unsupervised Dense Information Retrieval with Contrastive Learning}, 
  author={Gautier Izacard and Mathilde Caron and Lucas Hosseini and Sebastian Riedel and Piotr Bojanowski and Armand Joulin and Edouard Grave},
  year={2021},
  url = {https://arxiv.org/abs/2112.09118},
  doi = {10.48550/ARXIV.2112.09118},
}
@misc{vqav2,
      title={Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering}, 
      author={Yash Goyal and Tejas Khot and Douglas Summers-Stay and Dhruv Batra and Devi Parikh},
      year={2017},
      eprint={1612.00837},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1612.00837}, 
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{dosovitskiy2021an,
  added-at = {2023-06-22T17:58:14.000+0200},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  biburl = {https://www.bibsonomy.org/bibtex/2242f2231f90af37d7169530db3da4375/andolab},
  booktitle = {International Conference on Learning Representations},
  interhash = {325eaaeb3466512b4b887cc143bde420},
  intrahash = {242f2231f90af37d7169530db3da4375},
  keywords = {Transformer ViT},
  timestamp = {2023-06-22T17:58:14.000+0200},
  title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  url = {https://openreview.net/forum?id=YicbFdNTTy},
  year = 2021
}

@article{t5raffeletal2020,
    author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
    title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
    year = {2020},
    issue_date = {January 2020},
    publisher = {JMLR.org},
    volume = {21},
    number = {1},
    issn = {1532-4435},
    abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
    journal = {J. Mach. Learn. Res.},
    month = {jan},
    articleno = {140},
    numpages = {67},
    keywords = {transfer learning, natural language processing, multi-task learning, attention based models, deep learning}
}

@inproceedings{10.5555/3540261.3542249,
    author = {Sachan, Devendra Singh and Reddy, Siva and Hamilton, William and Dyer, Chris and Yogatama, Dani},
    title = {End-to-end training of multi-document reader and retriever for open-domain question answering},
    year = {2024},
    isbn = {9781713845393},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {We present an end-to-end differentiable training method for retrieval-augmented open-domain question answering systems that combine information from multiple retrieved documents when generating answers. We model retrieval decisions as latent variables over sets of relevant documents. Since marginalizing over sets of retrieved documents is computationally hard, we approximate this using an expectation-maximization algorithm. We iteratively estimate the value of our latent variable (the set of relevant documents for a given question) and then use this estimate to update the retriever and reader parameters. We hypothesize that such end-to-end training allows training signals to flow to the reader and then to the retriever better than stage-wise training. This results in a retriever that is able to select more relevant documents for a question and a reader that is trained on more accurate documents to generate an answer. Experiments on three benchmark datasets demonstrate that our proposed method outperforms all existing approaches of comparable size by 2-3 absolute exact match points, achieving new state-of-the-art results. Our results also demonstrate the feasibility of learning to retrieve to improve answer generation without explicit supervision of retrieval decisions.},
    booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
    articleno = {1988},
    numpages = {14},
    series = {NIPS '21}
}
@article{vqae,
  title={VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions},
  author={Li, Qing and Tao, Qingyi and Joty, Shafiq and Cai, Jianfei and Luo, Jiebo},
  journal={ECCV},
  year={2018}
}
@misc{aokvqa,
      title={A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge}, 
      author={Dustin Schwenk and Apoorv Khandelwal and Christopher Clark and Kenneth Marino and Roozbeh Mottaghi},
      year={2022},
      eprint={2206.01718},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2206.01718}, 
}
@misc{flava,
      title={FLAVA: A Foundational Language And Vision Alignment Model}, 
      author={Amanpreet Singh and Ronghang Hu and Vedanuj Goswami and Guillaume Couairon and Wojciech Galuba and Marcus Rohrbach and Douwe Kiela},
      year={2022},
      eprint={2112.04482},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2112.04482}, 
}

@misc{bertscore,
      title={BERTScore: Evaluating Text Generation with BERT}, 
      author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
      year={2020},
      eprint={1904.09675},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.09675}, 
}

@misc{qwenVL,
      title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution}, 
      author={Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Yang Fan and Kai Dang and Mengfei Du and Xuancheng Ren and Rui Men and Dayiheng Liu and Chang Zhou and Jingren Zhou and Junyang Lin},
      year={2024},
      eprint={2409.12191},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.12191}, 
}

@misc{miniCPM,
      title={MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies}, 
      author={Shengding Hu and Yuge Tu and Xu Han and Chaoqun He and Ganqu Cui and Xiang Long and Zhi Zheng and Yewei Fang and Yuxiang Huang and Weilin Zhao and Xinrong Zhang and Zheng Leng Thai and Kaihuo Zhang and Chongyi Wang and Yuan Yao and Chenyang Zhao and Jie Zhou and Jie Cai and Zhongwu Zhai and Ning Ding and Chao Jia and Guoyang Zeng and Dahai Li and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2404.06395},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.06395}, 
}

@misc{phi-3-vision,
      title={Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone}, 
      author={Marah Abdin and Jyoti Aneja and Hany Awadalla and Ahmed Awadallah and Ammar Ahmad Awan and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Jianmin Bao and Harkirat Behl and others},
      year={2024},
      eprint={2404.14219},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.14219}, 
}

@misc{revilm,
      title={Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning}, 
      author={Zhuolin Yang and Wei Ping and Zihan Liu and Vijay Korthikanti and Weili Nie and De-An Huang and Linxi Fan and Zhiding Yu and Shiyi Lan and Bo Li and Ming-Yu Liu and Yuke Zhu and Mohammad Shoeybi and Bryan Catanzaro and Chaowei Xiao and Anima Anandkumar},
      year={2023},
      eprint={2302.04858},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2302.04858}, 
}

@misc{mteb,
      title={MTEB: Massive Text Embedding Benchmark}, 
      author={Niklas Muennighoff and Nouamane Tazi and Loïc Magne and Nils Reimers},
      year={2023},
      eprint={2210.07316},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.07316}, 
}
@misc{whoops-2023,
      title={Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images}, 
      author={Nitzan Bitton-Guetta and Yonatan Bitton and Jack Hessel and Ludwig Schmidt and Yuval Elovici and Gabriel Stanovsky and Roy Schwartz},
      year={2023},
      eprint={2303.07274},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2303.07274}, 
}

@misc{vinvl,
      title={VinVL: Revisiting Visual Representations in Vision-Language Models}, 
      author={Pengchuan Zhang and Xiujun Li and Xiaowei Hu and Jianwei Yang and Lei Zhang and Lijuan Wang and Yejin Choi and Jianfeng Gao},
      year={2021},
      eprint={2101.00529},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2101.00529}, 
}

@misc{oscar,
      title={Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks}, 
      author={Xiujun Li and Xi Yin and Chunyuan Li and Pengchuan Zhang and Xiaowei Hu and Lei Zhang and Lijuan Wang and Houdong Hu and Li Dong and Furu Wei and Yejin Choi and Jianfeng Gao},
      year={2020},
      eprint={2004.06165},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2004.06165}, 
}

@article{symmetriccrossentropy,
  author       = {Yisen Wang and
                  Xingjun Ma and
                  Zaiyi Chen and
                  Yuan Luo and
                  Jinfeng Yi and
                  James Bailey},
  title        = {Symmetric Cross Entropy for Robust Learning with Noisy Labels},
  journal      = {CoRR},
  volume       = {abs/1908.06112},
  year         = {2019},
  url          = {http://arxiv.org/abs/1908.06112},
  eprinttype    = {arXiv},
  eprint       = {1908.06112},
  timestamp    = {Tue, 12 Jul 2022 11:20:47 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1908-06112.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{generalizedcrossentropy,
  title={Generalized cross entropy loss for training deep neural networks with noisy labels},
  author={Zhang, Zhilu and Sabuncu, Mert},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}




@misc{zhang2025jasperstelladistillationsota,
      title={Jasper and Stella: distillation of SOTA embedding models}, 
      author={Dun Zhang and Jiacheng Li and Ziyang Zeng and Fulong Wang},
      year={2025},
      eprint={2412.19048},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2412.19048}, 
}

@inproceedings{aditya2019spatial,
  title={Spatial knowledge distillation to aid visual reasoning},
  author={Aditya, Somak and Saha, Rudra and Yang, Yezhou and Baral, Chitta},
  booktitle={2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={227--235},
  year={2019},
  organization={IEEE}
}

@inproceedings{aditya2018combining,
  title={Combining knowledge and reasoning through probabilistic soft logic for image puzzle solving},
  author={Aditya, Somak and Yang, Yezhou and Baral, Chitta and Aloimonos, Yiannis},
  booktitle={Uncertainty in artificial intelligence},
  year={2018}
}