\subsection{Quality of the LLM Generated Explanations (Without Label and With Label)}
In our experiment, we observed that the quality of LLM-generated explanations, when compared to the ground truth, presented some interesting challenges. The BLEU scores (from 1 to 4) and ROUGE scores were not particularly high, which can be attributed to the differences in length between the generated explanations and the ground truth. The ground truth explanations tend to be quite concise, while the LLM-generated ones are of medium length. This mismatch in length leads to lower n-gram overlap, which in turn impacts the BLEU and ROUGE scores, reflecting a potential n-gram issue. However, when we assessed the cosine similarity scores, we found that the scores were consistently above 0.5 across all splits of both datasets. This suggests that despite the differences in length and n-gram overlap, the LLM-generated explanations still capture a significant amount of semantic similarity to the ground truth. The higher cosine scores indicate that, on a deeper level, the generated explanations align well with the intended meaning or context, even if they differ in phrasing or structure from the ground truth. This highlights the importance of considering both surface-level metrics like BLEU and ROUGE, as well as more nuanced measures like cosine similarity, to fully understand the quality of the generated explanations.


% \subsection{Cross-dataset Knowledge Transfer Abilities}
% We examined how effectively these sVLM architectures transfer knowledge to a target dataset after being trained on a different source dataset. Initially, we trained the models on the source dataset and then fine-tuned them on the target dataset for 1-2 epochs, utilizing randomly selected subsets comprising 10\%, 25\%, 50\%, and 100\% of the target data. This approach ensured that the task-specific layers were appropriately tuned to the new dataset. Our findings indicate that the models trained on a source dataset rich in variety and encompassing a broad range of knowledge performed well on the target dataset. Conversely, models trained on a source dataset lacking in diversity and comprehensive knowledge demonstrated poor performance on the target dataset. We reported our findings and scores in \ref{tab:cross_dataset_transfer_table} This underscores the importance of a diverse and knowledge-rich training dataset for enhancing the generalization abilities of sVLM architectures across different tasks and domains. The result in \ref{tab:cross_dataset_transfer_table} suggests the model's ability to leverage pre-learned knowledge from one domain can significantly enhance its adaptability and performance in another, thereby showcasing the potential of transformer models in multi-domain applications.


We first show the performance of these Transformer models on the two VQA based datasets when the external knowledge is integrated into each of the hypothesis by just a string concatenation. We used a "[SEP]" token as a separator between the external fact and hypothesis/Question. 
\textbf{Color-based questions:} Our analysis shows that for questions which require reasoning and understanding of colors ViLT performs poorly. For instance "what color is the object that is in the sky and is a type of aircraft ?", "which color is the thing that can continue to grow ?", "What object in gray is a type of electronic device ?" as in \ref{fig:colorQA}. CRIC contains roughly around 30428 questions involving colors, among which ViLT could only rightly classified 21734 questions making the color questions accuracy alone 54.98\%. Even after finetuning specifically on color-based-questions we achieved a best accuracy of 71.42\%. And for the SNLI-VE the best accuracy we could achieve is only 58.73\% on color based samples from the test set. And 60.93\% is the best accuracy recorded on the val set.

\textbf{Commonsense-based questions:} These questions demand a deeper understanding of the world, including general knowledge, reasoning abilities, and the capacity to infer implicit information like recognizing inherent properties of objects, like understanding that water is wet, fire is hot, and ice is cold. ViLT faces significant challenge in correctly answering commonsense-based questions. ViLT pretraining data might not encompass all the facets of commonsense knowledge that humans possess. It struggle with questions that require implicit understanding or assumptions about real-world scenarios. As per our current experimental setup, it does not have access to external world knowledge sources, such as structured knowledge graphs or databases, which are crucial for resolving commonsense-based questions that go beyond the scope of its pre-trained data. Around 27277 commonsense-based questions are present in CRIC, amongst which ViLT could classify questions correctly with an accuracy of 72.55\%. For e-SNLI-VE, although the count of questions (hypotheses) requiring commonsense reasoning to resolve is not known, from the qualitative analysis of a subset of predictions it becomes that clear that ViLT struggles to give correct predictions on such samples.


% \sa{Please do not write these things ever!}
% \subsection{Performance of Architectures with Additional knowledge from LLM (Without label and With label)}
% The observed discrepancy in performance in the VQA task when using LLAMA3 to generate explanations is intriguing and highlights the significance of context in generating high-quality responses. When LLAMA3 is provided with the answer to a question, the model can leverage this specific information to tailor its explanation more accurately and relevantly. The answer acts as a guiding anchor, enabling the model to focus on justifying or elaborating on why a particular answer is correct, thereby producing a well-targeted and high-quality explanation. This focused approach contributes to the high VQA accuracy of 97.91\%. In contrast, when the LLM is not given the answer and is tasked with generating an explanation from scratch, the model lacks a concrete reference point. Without the answer, the explanations can become more generic or vague, as the model must infer the relevance and correctness of various pieces of information without a clear direction. This uncertainty and lack of specific guidance can lead to explanations that do not effectively convey the rationale behind the answer, resulting in a significant drop in accuracy to 74.67\%.

%Majority of the experiments of knowledge integration from external sources like wikipedia, conceptnet has been done on the NLP spaces \cite{passageRetrieval}\cite{retreivalFromWiki}. 

%\hlgreen{Should go into contribution} We are amongst the very few who have tried to perform the knowledge integration process in a multi-modal space in a VQA setting keeping in mind the various potential sources to fetch knowledge e.g objects detected, scene graphs, Image Captions etc.