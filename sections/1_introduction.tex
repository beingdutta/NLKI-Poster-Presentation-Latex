\section{Introduction}
\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{images/paper-plot.png} 
\caption{Comparison of accuracy and model size for various small Vision-Language Models (sVLMs) AOKVQA (Val). $EKI$: Type-5 Explanation Knowledge Integrated models, as defined in \ref{tab:main_accuracy_table}, while $RKI$: Retrieved Knowledge Integrated models. The colour gradient represents execution time, with red indicating longer durations and blue indicating shorter ones.}
\label{fig:accuracy-vs-size}
\end{figure}
Early Visual Question Answering (VQA) work already recognised that images and questions alone are often insufficient and called for the use of external knowledge sources \cite{kbvqawang2015explicit,aditya2018explicit,aditya2019integrating}. Factoid VQA now routinely benefits from graph‐ or web‐scale retrieval \cite{fvqa2018,okvqaMarino_2019_CVPR,chen2024knowledge}, yet the commonsense variant remains under-explored, especially for \emph{small} vision–language models (sVLMs) such as \textsc{ViLT}, \textsc{VisualBERT} and \textsc{FLAVA}. Recent attempts either bypass pre-trained small VLMs \cite{ye2023improvingdance} or limit themselves to factual QA \cite{lin-byrne-2022-retrieval-ravqa,yang-etal-2023-vilm}. A big challenge in commonsense knowledge retrieval is a lack of a unified source -- no single knowledge graph, corpus, or model covers everyday physics, social conventions, and object affordances. Large language models (LLMs) have emerged as promising but noisy reservoirs of such knowledge \cite{ghosal-etal-2023-language}. These challenges motivate our four research questions: \textbf{RQ1)} \emph{to what extent can commonsense facts retrieved from knowledge bases and LLM-generated explanations help small VLMs?}; And \textbf{RQ2)} \emph{is one better suited than the other for commonsense reasoning in VQA systems?}; \textbf{RQ3)} \emph{can noise-robust losses mitigate the pervasive label noise in the context of external knowledge augmentation?}; and to draw relevance to recent progress in lightweight generative AI, we analyse \textbf{RQ4)} \emph{how do medium-size generative VLMs ($\leq 4B$) fare in commonsense tasks, in comparison to sVLMs (with or without knowledge integration)?}

To probe \textbf{RQ1–2}, we test the three sVLMs above ($\leq 240 M$ parameters) on CRIC and AOKVQA \cite{cric,aokvqa} along with a visual entailment dataset, e-SNLI-VE \cite{snlive}. Using ground-truth explanations (along with the image and question) lifts accuracy by up to \emph{10 to 12\%}, signalling the necessity and scope for utilising text-based knowledge. We therefore contrast (\emph{i}) ColBERTv2-retrieved facts from commonsense Knowledge Bases (KBs) and (\emph{ii}) free-form textual explanation produced by Llama-3-8B from dense/region captions, list of objects, and retrieved facts. A richer visual context helps curb hallucination, while simple architectural tweaks temper the effect of occasional noisy snippets. Because label noise dominates these datasets, \textbf{RQ3} tests whether Generalised Cross-Entropy (GCE) \cite{generalizedcrossentropy} and Symmetric Cross-Entropy (SCE) \cite{symmetriccrossentropy} preserve the gains and further boost the performance of knowledge integration, while mitigating the effects of label noise. Finally, addressing \textbf{RQ4}, we evaluate Qwen-2, Phi-3-Vision, MiniCPM and SmolVLM ($\leq 4 B$) and show that they too lack commonsense without explicit knowledge integration, underscoring the urgency of lightweight, knowledge-aware methods. Overall, we make the following contributions: 

\begin{compactitem}
\item \textbf{Plug-and-Play Knowledge Integration (NLKI)}: We introduce \textbf{N}atural-\textbf{L}anguage \textbf{K}nowledge \textbf{I}ntegration (NLKI) -- a plug-and-play framework that combines a retriever, an LLM explainer, and a lightweight reader with any sub-$240M$ VLM, so each module can be analysed and improved independently to improve the overall accuracy of small VLMs in commonsense reasoning tasks.
\item \textbf{Dense retrieval benchmark}: Compared to various state-of-the-art retrievers, we showed that finetuning ColBERTv2 using contrastive learning delivers the highest recall in retrieving the most relevant commonsense facts for CRIC, AOKVQA, and e-SNLI-VE queries.
\item \textbf{Accurate LLM Explanation using Context and Knowledge-Enriched Prompting}: We show that adding object and region information of an image (through dense/region captions generated from Florence-2-large) along with the retrieved facts to the Llama-3.1-8B prompt sharply reduces noise and hallucinated details in the generated explanation, compared to a caption-only context. 
\item \textbf{Knowledge \& noise-robustness gains}: LLM-generated explanations coupled with Symmetric Cross Entropy (SCE) or Generalised Cross-Entropy (GCE) preserve most of that gain under heavy label noise scenarios, raising AOKVQA accuracy by an average of $13.6\%$ and CRIC, e-SNLI-VE by $2-4\%$ across three architectures;
\item \textbf{Scale comparison}: Evaluations of Qwen-2, Phi-3-Vision, MiniCPM, and SmolVLM ($\leq 4 B$) show these larger models still lack commonsense, while NLKI-equipped small VLMs ($\leq 240 M$) can match or outperform them in some cases.
\end{compactitem}
