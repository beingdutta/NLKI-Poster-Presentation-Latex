\section{Knowledge-Augmented Visual QA framework}

Here, we explore two types of knowledge sources: traditional commonsense knowledge corpora (textual versions) and pre-trained Large Language Models. Following \citet{chen2024knowledge}, we can represent a retrieval or knowledge-augmented VQA system probabilistically as follows:
\begin{equation*}
    p(A|I,Q,\mathcal{K}) = 
    \begin{aligned}[t]
        &q_\Theta(\mathcal{F}_{ret}|Q,I,\mathcal{K}) \\
        &\quad p_\Phi(A|Q,I,\mathcal{F}_{ret})
    \end{aligned}
\end{equation*}
where, $q_\Theta(\cdot)$ is termed as a retriever, which is tuned to retrieve top $k$ relevant facts ($\mathcal{F}_{ret}$), and $p_\Phi(\cdot)$ is termed as the reader, which utilizes the question, image, and retrieved facts to predict the answer. To generalise to LLM-generated explanations, we can think of the reader as a model that outputs a paragraph relevant to the question. We will utilise the following notations wherever required:
\begin{equation*}
    p(A|I,Q,\mathcal{M}) = 
    \begin{aligned}[t]
        &q_\Theta(\mathcal{E}_{ret}|Q,I,\mathcal{M}) \\
        &\quad p_\Phi(A|Q,I,\mathcal{E}_{ret})
    \end{aligned}
\end{equation*}
where $\mathcal{M}$ may stand for a knowledge base or an LLM. $\mathcal{E}_{ret}$ may stand for a set of retrieved facts and LLM-generated explanation(s).

\subsection{Integrating Knowledge from Commonsense Corpus} 

\paragraph{Commonsense Corpus.}
Commonsense knowledge comprises basic facts and anecdotes about everyday objects, actions, and events, enabling humans to make inferences \cite{li2022survey}. Since Transformer models reason well with text-based commonsense knowledge \cite{ruletakerclark20}, we adopt Natural Language Knowledge Integration (NLKI), which is more flexible than structured knowledge graphs. \citet{raco} introduced a $20$M-fact natural language commonsense corpus from multiple human-annotated sources and web data. Based on initial ablations (Appendix Tables \ref{tab:cric_omcs_vs_20M_corpus} and \ref{tab:esnli_omcs_vs_20M_corpus}), we selected a subset: Open Mind Commonsense (OMCS) \cite{omcshavasi2010} for its higher similarity scores with ground-truth explanations and its compact size ($\sim$1.5M facts) describing everyday events and objects.

\paragraph{Commonsense Knowledge Retrieval.} Given an image-question pair, the goal is to retrieve $k$ relevant commonsense facts ($f_{i\leq k}$) from the commonsense corpus ($\mathcal{K}$) to fill in missing contextual knowledge. Ideally, retrieval involves abductive reasoning to infer necessary deductions. We primarily explore text-based dense retrieval using SBERT \cite{sbert}, ColBERTv2 \cite{colbert}, and  Stella selected from the MTEB leaderboard \cite{mteb}.

We represent the query in the following ways: 
\begin{compactitem}
    \item \textbf{Question}: We use the question as the query.
    \item \textbf{Caption + Question}: We generated image captions using BLIP-2 \cite{blip2}. We prepend the caption to the question to form the query.
    \item \textbf{Objects + Question}: We use a pre-trained YOLOv8 \cite{yolo} to detect objects from the image. We use the list of detected objects along with the question as the query.
    \item \textbf{Scene Graph Text + Question}: We generated scene graph from images using Relation Transformers \cite{cong2023reltr}. We prepend the sequence of scene graph triplets to the question as the query.
\end{compactitem}
We use SBERT \cite{sbert} to embed the commonsense corpus ($\mathcal{K}$), indexing it with FAISS \cite{faiss}. Queries (e.g., question/hypothesis) are encoded using the same model, and Nearest Neighbour Search retrieves the top-$k$ closest facts. Since SBERT captures semantic similarity, we further employ ColBERTv2, optimised for retrieval. Improved results (Table \ref{tab:retrieval_scores}) led us to fine-tune ColBERTv2 for commonsense fact retrieval.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\columnwidth, height=6.5cm]{images/pt_vs_ft_faiss_combined_final.jpg} 
    \caption{Facts retrieved by pre-trained ColBERTv2 vs. Finetuned ColBERTv2 vs. FAISS. Facts retrieved using pre-trained ColBERTv2 are more semantically close and contextually relevant to the query compared to the FAISS similarity search over SBERT embeddings.}
    \label{fig:pt_vs_ft_colbert}
\end{figure}

\paragraph{Finetuning ColBERTv2.} As shown in Fig.~\ref{fig:pt_vs_ft_colbert}, pre-trained ColBERTv2 retrieves partially relevant facts since it is trained on generalist datasets (MS-MARCO, TREC-CAR). To improve relevance, we fine-tune it following \citet{colbert}, formatting the commonsense corpus into triples \(\langle q, d^+, d^- \rangle\) where $q$ is the query, $d^+$ is the ground-truth knowledge, and $d^-$ is a randomly sampled fact from the corpus (see Appendix \ref{sec:finetuning-colbert} for finetuning details).

\paragraph{Commonsense Knowledge Integration.}
The retriever-reader architectures can be trained in two ways: (\textit{i}) in an end-to-end fashion where the retriever and the VLM are finetuned together \cite{10.5555/3540261.3542249}, and (\textit{ii}) where they are finetuned independently of each other. For simplicity, we go with the latter approach to improve both independently, as this allows for a modular plug-and-play framework wherein the retriever/reader can be switched as per the specific task. And in the case of sub-optimal retrieval performance, the reader ($p_\Phi(\cdot)$) is expected to learn to reason with noisy added knowledge.

\subsection{Integrating LLM-generated Explanations}
\label{sec:explanationtypes}
Using the image $I$, we extract various pieces of information such as multiple types of captions, a list of objects, and a scene graph to textually represent the visual context, which is also sometimes paired with the retrieved facts (these can be perceived as parameters to the prompt). To define an oracle setting (\textbf{Type 0}), we also supply the ground-truth label (GT-L). Interestingly, Llama-3.1-8B produces high-quality explanations when provided with the ground-truth label. Manual analysis shows that such explanations are riddled with hallucinations when the captions do not capture enough question-relevant information or the label is noisy in the case of Type-0 (See Fig. \ref{fig:hallucinated_explanation_collection}). Therefore, we performed a detailed analysis of the effect of varying all of the above parameters in generating an explanation.  We provide the common instruction in Fig.~\ref{prompt}, as utilised for the instruction-finetuned LLMs (such as Llama-3.1-8B) to generate explanations\footnote{For Llama-3.1-8B we used the official implementation from Huggingface at \href{https://huggingface.co/meta-llama/Llama-3.1-8B}{\url{https://huggingface.co/meta-llama/Llama-3.1-8B}}}. The variants are the following: 
 \begin{compactitem}
 \item \textbf{Type-0}: TC + Q + $RF_{I,Q}$ + GT-L
     \item \textbf{Type-1}: TC + Q + $RF_{I,Q}$
     \item \textbf{Type-2}: DC + Q + $RF_{I,Q}$
     \item \textbf{Type-3}: RC + Q + $RF_{I,Q}$
     \item \textbf{Type-4}: RC + Q + O + $RF_{I,Q}$
     \item \textbf{Type-5}: DC + RC + Q + O + $RF_{I,Q}$
     \item \textbf{Type-6}: DC + RC + Q + O (Type 5 without knowledge facts).
     \item {\textbf{Type-7}: Florence Finetuned Explanations},
 \end{compactitem}
where $TC$: Traditional Image Caption, $DC$: Dense Caption, $RC$: Region Based Caption, $Q$: Question, $RF$: Retrieved Facts, $O$: Objects, GT-L: Ground Truth Label.

\paragraph{Integrating Explanations.} Since explanations are more targeted and concise, we explore baseline variations by simply prepending the explanation with the question for the small VLMs.
